Project Title
Retrieval Augmented Generation with Elasticsearch and OpenAI

Introduction
This project demonstrates how to implement a Retrieval Augmented Generation (RAG) system by combining Elasticsearch for semantic search with OpenAI's GPT models for generating responses. The application indexes a dataset of Wikipedia articles into Elasticsearch, performs semantic search using OpenAI embeddings, and generates answers to user queries using OpenAI's Chat Completion API.

Theory
Retrieval Augmented Generation (RAG)
Retrieval Augmented Generation is a technique that enhances the capabilities of language models by providing them with external context retrieved from a knowledge base or document store. Instead of relying solely on the model's internal parameters, RAG systems retrieve relevant information in response to a query and use it to generate more accurate and up-to-date answers.

Components
Elasticsearch: An open-source, distributed search and analytics engine capable of storing and searching large volumes of data quickly. It supports vector similarity search, making it suitable for semantic search applications.

OpenAI Embeddings: High-dimensional vector representations of text generated by OpenAI's embedding models. These embeddings capture semantic meaning, allowing for similarity comparisons between texts.

OpenAI Chat Completion API: Provides access to OpenAI's language models (like GPT-3.5-turbo) for generating human-like text based on input prompts.

Operation Process
1. Environment Setup
The project uses Docker and Docker Compose to orchestrate the application components:

Elasticsearch: Runs in a Docker container, configured for single-node operation.
Kibana (Optional): Provides a user interface for interacting with Elasticsearch data.
Application: A Python script that performs data ingestion, semantic search, and response generation.
2. Data Ingestion
Dataset: A pre-embedded dataset of Wikipedia articles is downloaded and extracted.
Indexing: The dataset is indexed into Elasticsearch, where each document includes text, metadata, and vector embeddings.
3. Semantic Search
Query Embedding: A user query is converted into an embedding using OpenAI's embedding model.
k-NN Search: Elasticsearch performs a k-Nearest Neighbors (k-NN) search using the query embedding to find the most relevant documents based on vector similarity.
4. Response Generation
Context Retrieval: The top search result is extracted to serve as context.
OpenAI Chat Completion: The user query and the retrieved context are sent to OpenAI's Chat Completion API, which generates a coherent and informed response.
Instructions
Prerequisites
Docker and Docker Compose installed on your system.
An OpenAI API key.
Setup and Run
Clone the Repository

git clone https://github.com/chensihai/RAG_Elastic_Search
cd RAG_Elastic_Search
Configure Environment Variables

Copy the sample environment file and update it with your OpenAI API key.

cp .env.sample .env
Edit .env and set OPENAI_API_KEY:

env
Copy code
# .env
OPENAI_API_KEY=sk-your-openai-api-key
Build and Start the Docker Containers

docker-compose up -d --build
Monitor the Application

View logs to monitor the progress:

docker-compose logs -f app
The application will:

Download and extract the dataset.
Create the Elasticsearch index.
Index the data into Elasticsearch.
Perform a semantic search.
Generate an answer using OpenAI's Chat Completion API.
Access Kibana (Optional)

If you included Kibana in your docker-compose.yml, access it at http://localhost:5601.
Use Kibana to visualize and explore the indexed data.
Clean Up
To stop and remove the containers and associated volumes:

docker-compose down -v
File Structure
docker-compose.yml: Defines the services (Elasticsearch, Kibana, app) and their configurations.
Dockerfile: Specifies the application's Docker image, including dependencies.
requirements.txt: Lists the Python dependencies.
your_script.py: The main application script performing data ingestion, search, and response generation.
.env.sample: Sample environment variables file.
README.txt: Project documentation.
Notes
Elasticsearch Configuration: Security features are disabled for simplicity. For production use, enable security and configure authentication.
Memory Settings: Elasticsearch is configured with a 1GB JVM heap size. Adjust based on your system resources.
Data Persistence: Elasticsearch data is stored in a Docker volume to persist data across restarts.
OpenAI Usage: Ensure compliance with OpenAI's usage policies when using the API.
Troubleshooting
Connection Issues: Ensure that Elasticsearch is running and accessible by the application.
OpenAI Errors: Verify that the OpenAI API key is correct and has sufficient permissions.
Resource Constraints: Adjust memory settings if you encounter performance issues.
Additional Information
Elasticsearch Index Mapping
The Elasticsearch index is created with mappings to support vector fields for semantic search:

title_vector: Dense vector of the title embedding.
content_vector: Dense vector of the content embedding.
Adjusting the Query
You can modify the question variable in your_script.py to change the query:

python
Copy code
question = 'Your custom question here'
Modifying the Application
Feel free to adjust the script to:

Index different datasets.
Use different OpenAI models.
Implement additional functionality.
References
Elasticsearch Documentation: https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html
Kibana Documentation: https://www.elastic.co/guide/en/kibana/current/index.html
OpenAI API Documentation: https://platform.openai.com/docs/api-reference
OpenAI Cookbook Example: Retrieval Augmented Generation with Elasticsearch and OpenAI
License
This project is licensed under the MIT License - see the LICENSE file for details.
